@inproceedings{cai2024predicting,
 abstract = {Temporal Knowledge Graph (TKG) reasoning seeks to predict future incomplete facts leveraging historical data. While existing approaches have shown effectiveness in addressing the task through various perspectives, such as graph learning and logic rules, they are limited in capturing the indeterminacy in future events, particularly in the case of rare/unseen facts. To tackle the highlighted issues, we introduce a novel approach by conceptualizing TKG reasoning as a sequence denoising process for future facts, namely DiffuTKG. Concretely, we first encodes the historical events as the conditional sequence. Then we gradually introduce Gaussian noise to corrupt target facts during the forward process and then employ a transformer-based conditional denoiser to restore them in the reverse phase. Moreover, we introduce an uncertainty regularization loss to mitigate the risk of prediction biases by favoring frequent scenarios over rare/unseen facts. Empirical results on four real-world datasets show that DiffuTKG outperforms state-of-the-art methods across multiple evaluation metrics.},
 address = {Bangkok, Thailand and virtual meeting},
 author = {Cai, Yuxiang  and
Liu, Qiao  and
Gan, Yanglei  and
Li, Changlin  and
Liu, Xueyi  and
Lin, Run  and
Luo, Da  and
JiayeYang, JiayeYang},
 booktitle = {Findings of the Association for Computational Linguistics ACL 2024},
 editor = {Ku, Lun-Wei  and
Martins, Andre  and
Srikumar, Vivek},
 month = {August},
 pages = {5766--5778},
 publisher = {Association for Computational Linguistics},
 title = {Predicting the Unpredictable: Uncertainty-Aware Reasoning over Temporal Knowledge Graphs via Diffusion Process},
 url = {https://aclanthology.org/2024.findings-acl.343},
 year = {2024}
}
